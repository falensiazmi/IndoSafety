{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-28T20:15:29.282636Z",
     "iopub.status.busy": "2025-10-28T20:15:29.282351Z",
     "iopub.status.idle": "2025-10-28T20:15:48.776968Z",
     "shell.execute_reply": "2025-10-28T20:15:48.775783Z",
     "shell.execute_reply.started": "2025-10-28T20:15:29.282610Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install -U transformers huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T20:15:48.778451Z",
     "iopub.status.busy": "2025-10-28T20:15:48.778191Z",
     "iopub.status.idle": "2025-10-28T20:16:19.703914Z",
     "shell.execute_reply": "2025-10-28T20:16:19.703181Z",
     "shell.execute_reply.started": "2025-10-28T20:15:48.778426Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "from peft import PeftModel\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login with Hugging Face account (to access gated model such as Gemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T20:16:19.706629Z",
     "iopub.status.busy": "2025-10-28T20:16:19.706090Z",
     "iopub.status.idle": "2025-10-28T20:16:19.856905Z",
     "shell.execute_reply": "2025-10-28T20:16:19.856217Z",
     "shell.execute_reply.started": "2025-10-28T20:16:19.706604Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "login('YOUR_HF_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T20:16:19.858010Z",
     "iopub.status.busy": "2025-10-28T20:16:19.857708Z",
     "iopub.status.idle": "2025-10-28T20:16:20.805648Z",
     "shell.execute_reply": "2025-10-28T20:16:20.804843Z",
     "shell.execute_reply.started": "2025-10-28T20:16:19.857965Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Kaggle\n",
    "# FOLDER_NAME = '/kaggle/input/indosafety/'\n",
    "\n",
    "# Local\n",
    "FOLDER_NAME = 'dataset'\n",
    "\n",
    "EVAL1_PATH = os.path.join(FOLDER_NAME, 'IndoSafety-Eval-1.xlsx')\n",
    "EVAL2_PATH = os.path.join(FOLDER_NAME, 'IndoSafety-Eval-2.xlsx')\n",
    "\n",
    "df_eval_1 = pd.read_excel(EVAL1_PATH)\n",
    "df_eval_2 = pd.read_excel(EVAL2_PATH)\n",
    "df_eval_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T20:16:20.807083Z",
     "iopub.status.busy": "2025-10-28T20:16:20.806447Z",
     "iopub.status.idle": "2025-10-28T20:16:20.820038Z",
     "shell.execute_reply": "2025-10-28T20:16:20.819243Z",
     "shell.execute_reply.started": "2025-10-28T20:16:20.807058Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_inference(model_config:dict,\n",
    "                  generation_config:dict,\n",
    "                  dataset:pd.DataFrame, \n",
    "                  output_path:str,\n",
    "                  batch_size:int = 1,\n",
    "                  model_lora_path:str = None,\n",
    "                  langs:list = ['indonesian-formal', 'colloquial', 'minangkabau', 'java', 'sunda'], \n",
    "                  use_system_role:bool = True,\n",
    "                  debug:bool = False,\n",
    "                  ):\n",
    "    # load model and tokenizer\n",
    "    model_name = model_config['pretrained_model_name_or_path']\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        **model_config,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, revision=\"main\", trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # load LoRA weight (optional)\n",
    "    if model_lora_path is not None:\n",
    "        model = PeftModel.from_pretrained(base_model, model_lora_path)\n",
    "    else:\n",
    "        model = base_model\n",
    "    model.eval()\n",
    "\n",
    "    # run inference per language\n",
    "    for lang in langs:\n",
    "        print(f\"\\nðŸ”¹ Running inference for language: {lang}\")\n",
    "        results = []\n",
    "        batch = [dataset[lang][i:i + batch_size].tolist() for i in range(0, len(dataset[lang]), batch_size)]\n",
    "        for prompts in tqdm(batch, desc=f\"{lang}\"):\n",
    "            if use_system_role:\n",
    "                messages = [[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant. Answer questions according to the language used by the user.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt} ] for prompt in prompts]\n",
    "            else:\n",
    "                messages = [\n",
    "                    [{\"role\": \"user\", \"content\": prompt}] for prompt in prompts\n",
    "                ]\n",
    "            formatted_prompts = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            model_inputs = tokenizer(formatted_prompts, return_tensors=\"pt\", padding=True, padding_side='left').to(model.device)\n",
    "            with torch.no_grad():\n",
    "                generated_ids = model.generate(\n",
    "                    **model_inputs,\n",
    "                    **generation_config\n",
    "                )\n",
    "                generated_ids = [\n",
    "                    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "                ]\n",
    "                responses = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "                results.extend(responses)\n",
    "                \n",
    "            if debug:\n",
    "                results.extend([''] * (len(dataset)-len(results))) # fill with empty strings when debugging\n",
    "                break\n",
    "\n",
    "        # write output to an .xlsx file\n",
    "        df_new = dataset[['id','risk_area','types_of_harm','specific_harms',lang]].copy()\n",
    "        if isinstance(model, PeftModel):\n",
    "            # add prefix/suffix FT for finetuned model\n",
    "            output_filename = f'FT_result_{lang}_{model_name}'.replace(\"/\", \"_\").replace(\".\", \"_\") + '.xlsx'\n",
    "            df_new[f'{lang}_{model_name}-FT_response'] = results\n",
    "        else:\n",
    "            output_filename = f'result_{lang}_{model_name}'.replace(\"/\", \"_\").replace(\".\", \"_\") + '.xlsx'\n",
    "            df_new[f'{lang}_{model_name}_response'] = results\n",
    "        full_output_path = os.path.join(output_path, output_filename)\n",
    "        df_new.to_excel(full_output_path, index=False, engine=\"openpyxl\")\n",
    "\n",
    "    # clean up memory\n",
    "    try:\n",
    "        del model, tokenizer\n",
    "        torch.cuda.empty_cache()    # Free GPU memory\n",
    "        gc.collect()                # Run garbage collection\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Response (IndoSafety-Eval-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T20:16:20.821076Z",
     "iopub.status.busy": "2025-10-28T20:16:20.820815Z",
     "iopub.status.idle": "2025-10-28T20:16:20.842476Z",
     "shell.execute_reply": "2025-10-28T20:16:20.841501Z",
     "shell.execute_reply.started": "2025-10-28T20:16:20.821056Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "models = [\n",
    "    'Qwen/Qwen2.5-7B-Instruct',\n",
    "    'Qwen/Qwen2.5-14B-Instruct',\n",
    "    'meta-llama/Llama-3.1-8B-Instruct',\n",
    "    'sail/Sailor2-8B-Chat',\n",
    "    'aisingapore/Llama-SEA-LION-v3-8B-IT',\n",
    "    'SeaLLMs/SeaLLMs-v3-7B-Chat',\n",
    "    'GoToCompany/gemma2-9b-cpt-sahabatai-v1-instruct',\n",
    "    'google/gemma-2-9b-it',\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    pretrained_model_name_or_path = model\n",
    "    device_map = 'auto'\n",
    "    torch_dtype = torch.bfloat16\n",
    "    model_config = {'pretrained_model_name_or_path':pretrained_model_name_or_path,\n",
    "                    'device_map':device_map,\n",
    "                    'torch_dtype':torch_dtype}\n",
    "    \n",
    "    # set generation config\n",
    "    max_new_tokens = 2048\n",
    "    generation_config = {'max_new_tokens':max_new_tokens}\n",
    "    \n",
    "    # set other params\n",
    "    output_path = '1_responses'\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    batch_size = 8\n",
    "\n",
    "    # fill lang='prompt' for IndoSafety-Eval-1\n",
    "    langs = ['prompt']\n",
    "\n",
    "    # handle case when model does not use chat template\n",
    "    if model in ['google/gemma-2-9b-it']:\n",
    "        use_system_role=False\n",
    "    else:\n",
    "        use_system_role=True\n",
    "\n",
    "    # for debugging\n",
    "    debug = False\n",
    "    \n",
    "    run_inference(model_config=model_config,\n",
    "                  generation_config=generation_config,\n",
    "                  dataset=df_eval_1,\n",
    "                  output_path=output_path,\n",
    "                  batch_size=batch_size,\n",
    "                  langs=langs,\n",
    "                  use_system_role=use_system_role,\n",
    "                  debug=debug)\n",
    "    \n",
    "    # optional: remove model from disk when finished\n",
    "    # cache_dir = os.path.expanduser(\"~/.cache/huggingface/hub\")\n",
    "    # for folder in os.listdir(cache_dir):\n",
    "    #     if model.replace('/', '--') in folder:\n",
    "    #         shutil.rmtree(os.path.join(cache_dir, folder), ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Response (IndoSafety-Eval-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T20:16:20.843609Z",
     "iopub.status.busy": "2025-10-28T20:16:20.843374Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "models = [\n",
    "    'Qwen/Qwen2.5-7B-Instruct',\n",
    "    'Qwen/Qwen2.5-14B-Instruct',\n",
    "    'meta-llama/Llama-3.1-8B-Instruct',\n",
    "    'sail/Sailor2-8B-Chat',\n",
    "    'aisingapore/Llama-SEA-LION-v3-8B-IT',\n",
    "    'SeaLLMs/SeaLLMs-v3-7B-Chat',\n",
    "    'GoToCompany/gemma2-9b-cpt-sahabatai-v1-instruct',\n",
    "    'google/gemma-2-9b-it',\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    pretrained_model_name_or_path = model\n",
    "    device_map = 'auto'\n",
    "    torch_dtype = torch.bfloat16\n",
    "    model_config = {'pretrained_model_name_or_path':pretrained_model_name_or_path,\n",
    "                    'device_map':device_map,\n",
    "                    'torch_dtype':torch_dtype}\n",
    "    \n",
    "    # set generation config\n",
    "    max_new_tokens = 2048\n",
    "    generation_config = {'max_new_tokens':max_new_tokens}\n",
    "    \n",
    "    # set other params\n",
    "    output_path = '1_responses'\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    batch_size = 8\n",
    "\n",
    "    # only generate response for supported languages\n",
    "    if model in ['Qwen/Qwen2.5-7B-Instruct', 'Qwen/Qwen2.5-14B-Instruct', 'meta-llama/Llama-3.1-8B-Instruct', 'google/gemma-2-9b-it']:\n",
    "        langs = ['indonesian-formal', 'colloquial']\n",
    "    elif model in ['aisingapore/Llama-SEA-LION-v3-8B-IT', 'sail/Sailor2-8B-Chat', 'GoToCompany/gemma2-9b-cpt-sahabatai-v1-instruct']:\n",
    "        langs = ['indonesian-formal', 'colloquial', 'java', 'sunda']\n",
    "    elif model in ['SeaLLMs/SeaLLMs-v3-7B-Chat']:\n",
    "        langs = ['indonesian-formal', 'colloquial', 'java']\n",
    "    else: # default: all language variants\n",
    "        langs = ['indonesian-formal', 'colloquial', 'minangkabau', 'java', 'sunda']\n",
    "\n",
    "    # handle case when model does not use chat template\n",
    "    if model in ['google/gemma-2-9b-it']:\n",
    "        use_system_role=False\n",
    "    else:\n",
    "        use_system_role=True\n",
    "\n",
    "    # for debugging\n",
    "    debug = False\n",
    "    \n",
    "    run_inference(model_config=model_config,\n",
    "                  generation_config=generation_config,\n",
    "                  dataset=df_eval_2,\n",
    "                  output_path=output_path,\n",
    "                  batch_size=batch_size,\n",
    "                  langs=langs,\n",
    "                  use_system_role=use_system_role,\n",
    "                  debug=debug)\n",
    "    \n",
    "    # optional: remove model from disk when finished\n",
    "    # cache_dir = os.path.expanduser(\"~/.cache/huggingface/hub\")\n",
    "    # for folder in os.listdir(cache_dir):\n",
    "    #     if model.replace('/', '--') in folder:\n",
    "    #         shutil.rmtree(os.path.join(cache_dir, folder), ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Response After Finetuning (IndoSafety-Eval-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    'sail/Sailor2-8B-Chat',\n",
    "    'aisingapore/Llama-SEA-LION-v3-8B-IT',\n",
    "    'SeaLLMs/SeaLLMs-v3-7B-Chat',\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    pretrained_model_name_or_path = model\n",
    "    device_map = 'auto'\n",
    "    torch_dtype = torch.bfloat16\n",
    "    model_config = {'pretrained_model_name_or_path':pretrained_model_name_or_path,\n",
    "                    'device_map':device_map,\n",
    "                    'torch_dtype':torch_dtype}\n",
    "    \n",
    "    # select LoRA model\n",
    "    if model == 'sail/Sailor2-8B-Chat':\n",
    "        model_lora_path = 'finetuning/sail_Sailor2-8B-Chat_2025-05-09_08-58-25/model'\n",
    "    elif model == 'aisingapore/Llama-SEA-LION-v3-8B-IT':\n",
    "        model_lora_path = 'finetuning/aisingapore_Llama-SEA-LION-v3-8B-IT_2025-07-01_15-34-59/model'\n",
    "    elif model == 'SeaLLMs_SeaLLMs-v3-7B-Chat_2025-07-01_13-11-03':\n",
    "        model_lora_path = 'finetuning/SeaLLMs_SeaLLMs-v3-7B-Chat_2025-07-01_13-11-03/model'\n",
    "    \n",
    "    # set generation config\n",
    "    max_new_tokens = 2048\n",
    "    generation_config = {'max_new_tokens':max_new_tokens}\n",
    "    \n",
    "    # set other params\n",
    "    output_path = '1_responses'\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    batch_size = 8\n",
    "\n",
    "    # only generate response for supported languages\n",
    "    if model in ['Qwen/Qwen2.5-7B-Instruct', 'Qwen/Qwen2.5-14B-Instruct', 'meta-llama/Llama-3.1-8B-Instruct', 'google/gemma-2-9b-it']:\n",
    "        langs = ['indonesian-formal', 'colloquial']\n",
    "    elif model in ['aisingapore/Llama-SEA-LION-v3-8B-IT', 'sail/Sailor2-8B-Chat', 'GoToCompany/gemma2-9b-cpt-sahabatai-v1-instruct']:\n",
    "        langs = ['indonesian-formal', 'colloquial', 'java', 'sunda']\n",
    "    elif model in ['SeaLLMs/SeaLLMs-v3-7B-Chat']:\n",
    "        langs = ['indonesian-formal', 'colloquial', 'java']\n",
    "    else: # default: all language variants\n",
    "        langs = ['indonesian-formal', 'colloquial', 'minangkabau', 'java', 'sunda']\n",
    "\n",
    "    # handle case when model does not use chat template\n",
    "    if model in ['google/gemma-2-9b-it']:\n",
    "        use_system_role=False\n",
    "    else:\n",
    "        use_system_role=True\n",
    "\n",
    "    # for debugging\n",
    "    debug = False\n",
    "    \n",
    "    run_inference(model_config=model_config,\n",
    "                  generation_config=generation_config,\n",
    "                  dataset=df_eval_2,\n",
    "                  output_path=output_path,\n",
    "                  batch_size=batch_size,\n",
    "                  langs=langs,\n",
    "                  use_system_role=use_system_role,\n",
    "                  debug=debug,\n",
    "                  model_lora_path=model_lora_path)\n",
    "    \n",
    "    # optional: remove model from disk when finished\n",
    "    # cache_dir = os.path.expanduser(\"~/.cache/huggingface/hub\")\n",
    "    # for folder in os.listdir(cache_dir):\n",
    "    #     if model.replace('/', '--') in folder:\n",
    "    #         shutil.rmtree(os.path.join(cache_dir, folder), ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FOR TESTING\n",
    "\n",
    "# models = [\n",
    "#     'Qwen/Qwen2.5-0.5B-Instruct'\n",
    "# ]\n",
    "\n",
    "# for model in models:\n",
    "#     pretrained_model_name_or_path = model\n",
    "#     device_map = 'auto'\n",
    "#     torch_dtype = torch.bfloat16\n",
    "#     model_config = {'pretrained_model_name_or_path':pretrained_model_name_or_path,\n",
    "#                     'device_map':device_map,\n",
    "#                     'torch_dtype':torch_dtype}\n",
    "    \n",
    "#     # select LoRA model\n",
    "#     model_lora_path = 'finetuning/Qwen_Qwen2.5-0.5B-Instruct_2025-11-03_20-40-59/model'\n",
    "    \n",
    "#     # set generation config\n",
    "#     max_new_tokens = 2048\n",
    "#     generation_config = {'max_new_tokens':max_new_tokens}\n",
    "\n",
    "#     # set other params\n",
    "#     output_path = '1_responses'\n",
    "#     os.makedirs(output_path, exist_ok=True)\n",
    "#     batch_size = 16\n",
    "\n",
    "#     # only generate response for supported language\n",
    "#     langs = ['indonesian-formal', 'colloquial']\n",
    "\n",
    "#     # handle case when model does not use chat template\n",
    "#     if model in ['google/gemma-2-9b-it']:\n",
    "#         use_system_role=False\n",
    "#     else:\n",
    "#         use_system_role=True\n",
    "\n",
    "#     # for debugging\n",
    "#     debug = True\n",
    "    \n",
    "#     run_inference(model_config=model_config,\n",
    "#                   generation_config=generation_config,\n",
    "#                   dataset=df_eval_2,\n",
    "#                   output_path=output_path,\n",
    "#                   batch_size=batch_size,\n",
    "#                   langs=langs,\n",
    "#                   use_system_role=use_system_role,\n",
    "#                   debug=debug,\n",
    "#                   model_lora_path=model_lora_path)\n",
    "    \n",
    "#     # optional: remove model from disk when finished\n",
    "#     # cache_dir = os.path.expanduser(\"~/.cache/huggingface/hub\")\n",
    "#     # for folder in os.listdir(cache_dir):\n",
    "#     #     if model.replace('/', '--') in folder:\n",
    "#     #         shutil.rmtree(os.path.join(cache_dir, folder), ignore_errors=True)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8175205,
     "sourceId": 12919989,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
